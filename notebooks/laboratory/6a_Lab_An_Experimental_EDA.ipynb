{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6a_Lab_An_Experimental_EDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudio1975/SDS2020/blob/master/notebooks/laboratory/6a_Lab_An_Experimental_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlcrvwrX70Uv",
        "colab_type": "text"
      },
      "source": [
        "# **An Experimental Exploratory Data Analysis for a Classification Task step 6**\n",
        "\n",
        "### ***From Visualization to Statistical Analysis***\n",
        "\n",
        "### ***From Feature Engineering to Feature Selection***\n",
        "\n",
        "### ***From the Best Model Selection to Interpretability***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vngHrhnzA7qr",
        "colab_type": "text"
      },
      "source": [
        "To start the exploration set up the environment with libraries, upload the data set (it's stored in a github repository) and split it into target variable and features variables. No more set up is required using Google Colab. Look at the guidelines: https://colab.research.google.com/notebooks/welcome.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2o9mujoz5by",
        "colab_type": "text"
      },
      "source": [
        "#### **Contents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRwDUZ019MxF",
        "colab_type": "text"
      },
      "source": [
        "The goal of this challenge, launched by CrowdAnalytix, is to develop a model to predict whether a mortgage will be funded or not based on certain factors in a customer’s application data. \n",
        "The evaluation metric used is the F1 score.\n",
        "The data set is made up by 45.642 observations with predictor variables (21 features) and the target variable. It's a classification task with the goal to predict the 'Result' target variable for every row (Funded, Not Funded). Look at the competition: https://www.crowdanalytix.com/contests/propensity-to-fund-mortgages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9DkyQZ2Ls-4",
        "colab_type": "text"
      },
      "source": [
        "### **Exploratory Data Analysis (EDA) Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVQahUb-8995",
        "colab_type": "text"
      },
      "source": [
        "![](http://www.theleader.info/wp-content/uploads/2017/07/Mortgage-rates.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs9lEhRM0Mne",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJlI9RQG-A8w",
        "colab_type": "text"
      },
      "source": [
        "#####- Upload libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQysnLU9RUKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload libraries\n",
        "\n",
        "# to handle data set\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# to plot\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "# statistics\n",
        "import statistics\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import logit\n",
        "from scipy.stats import chi2_contingency\n",
        "from scipy.stats import kurtosis \n",
        "from scipy.stats import skew\n",
        "from statistics import stdev \n",
        "\n",
        "# to split data set \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# to build models\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# to evaluate models\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# to handle imbalanced data set\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "\n",
        "# feature engineering\n",
        "!pip install feature-engine\n",
        "import feature_engine\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# feature importance\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "!pip install eli5 \n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "!pip install shap\n",
        "import shap\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLFLLtY2-I06",
        "colab_type": "text"
      },
      "source": [
        "#####- Upload data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgQMY8HqDrGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload dataset\n",
        "url ='https://raw.githubusercontent.com/claudio1975/SDS2020/master/data/CAX_train_small.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2_fumrJ-NCl",
        "colab_type": "text"
      },
      "source": [
        "#####- Split data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwPBNz5pDvDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data set between target and features\n",
        "X_full = df\n",
        "y = X_full.RESULT\n",
        "X_full = X_full.drop(['RESULT'], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIEjdLM1BGti",
        "colab_type": "text"
      },
      "source": [
        "# Summarize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvXmGWII7I_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at dimension of data set and types of each attribute\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDB8l79ZLldm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summarize attribute distributions of the data frame\n",
        "df.describe(include='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl_waIVssaPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take a peek at the first rows of the data\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6N4eR9vEXQi",
        "colab_type": "text"
      },
      "source": [
        "Explanatory variables are grouped into categorical variables and numerical variables and for each one let's do a graphical and non-graphical analysis, but before this split let's run some data preparation activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqhoHvdpPKse",
        "colab_type": "text"
      },
      "source": [
        "# Formatting Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbHzfBaj2J5t",
        "colab_type": "text"
      },
      "source": [
        "If necessary, it's a good practice to format data, after have taken a peek of it. Missing values on numeric features are marked by \"-1\", meanwhile for categorical features they are marked with \"Unknown\"; let's imput these values with \"NA\".  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoC1p8FWLFbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replaced both '-1' and 'Unknown' values with NA's\n",
        "X_full[X_full== -1] = np.nan\n",
        "X_full[X_full==\"Unknown\"] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfr3gw7vQeWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Format data into float and object types and split mixed variables\n",
        "X_full['PROPERTY VALUE'] = X_full['PROPERTY VALUE'].astype(float)\n",
        "X_full['MORTGAGE PAYMENT'] = X_full['MORTGAGE PAYMENT'].astype(float)\n",
        "X_full['AMORTIZATION'] = X_full['AMORTIZATION'].astype(float)\n",
        "X_full['TERM'] = X_full['TERM'].astype(float)\n",
        "X_full['INCOME'] = X_full['INCOME'].astype(float)\n",
        "X_full['INCOME TYPE'] = X_full['INCOME TYPE'].astype(object)\n",
        "X_full['CREDIT SCORE'] = X_full['CREDIT SCORE'].astype(float)\n",
        "X_full['FSA_num'] = X_full['FSA'].str.extract('(\\d+)') # extract numerical part\n",
        "X_full['FSA_let'] = X_full['FSA'].str[0] # extract the first letter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa67IEEHGdPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rename some features for a practical use\n",
        "X_full = X_full.rename(columns={\"MORTGAGE PURPOSE\":\"MORTGAGE_PURPOSE\",\"PAYMENT FREQUENCY\":\"PAYMENT_FREQUENCY\",\"PROPERTY TYPE\":\"PROPERTY_TYPE\",\"AGE RANGE\":\"AGE_RANGE\",\"PROPERTY VALUE\": \"PROPERTY_VALUE\",\n",
        "                                \"MORTGAGE PAYMENT\": \"MORTGAGE_PAYMENT\", \"MORTGAGE AMOUNT\":\"MORTGAGE_AMOUNT\",\"INCOME TYPE\":\"INCOME_TYPE\",\"CREDIT SCORE\":\"CREDIT_SCORE\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJDCLUOUl-dv",
        "colab_type": "text"
      },
      "source": [
        "# Handling missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6F1o-jDnAp_",
        "colab_type": "text"
      },
      "source": [
        "There are two categorical features with missing values lower than 40%. The approach followed: filled up missing values with the mode of each variable. With large percentage of missing values (>=15%) it's suggested to add a \"missing indicator\", a boolean variable with 1/true (missing value) or 0/false (actual value).  \"[Pawel Grabinski](https://www.kdnuggets.com/2018/12/feature-engineering-explained.html)\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPRuA3QAmGr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check missing values both to numeric features and categorical features\n",
        "X_full.isnull().sum()/X_full.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "espKgZhvnJiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input missing values with median or mode depending of features class\n",
        "X_full['GENDER'].fillna(X_full['GENDER'].mode()[0], inplace=True)\n",
        "X_full['GENDER'] = X_full['GENDER'].astype(object)\n",
        "X_full['INCOME_TYPE'].fillna(X_full['INCOME_TYPE'].mode()[0], inplace=True)\n",
        "X_full['INCOME_TYPE'] = X_full['INCOME_TYPE'].astype(object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEut5Q20nL6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# final check\n",
        "X_full.isnull().sum()/X_full.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQcW8rYbOFt6",
        "colab_type": "text"
      },
      "source": [
        "# Target Variable Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6lQf-z8OIIl",
        "colab_type": "text"
      },
      "source": [
        "The target variable is grouped into two classes: \"Funded\" and \"Not Funded\". Looking at the barplot, it's quite imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfJ91jxjOLJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summarize the class distribution \n",
        "count = pd.crosstab(index = y, columns=\"count\")\n",
        "percentage = pd.crosstab(index = y, columns=\"frequency\")/pd.crosstab(index = y, columns=\"frequency\").sum()\n",
        "pd.concat([count, percentage], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EahRBbD7QTOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Plot the target variable\n",
        "ax = sns.countplot(x=y, data=X_full, order=[\"FUNDED\", \"NOT FUNDED\"]).set_title(\"Target Variable Distribution\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqcTsDB4OkMK",
        "colab_type": "text"
      },
      "source": [
        "# Categorical Features Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mod4f0ZSOhnk",
        "colab_type": "text"
      },
      "source": [
        "#####- Analysis for categorical features (barplot, univariate analysis, bivariate analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZQKBWaMOaLB",
        "colab_type": "text"
      },
      "source": [
        "Let's group all categorical features into a new subset: let's run a graphical analysis using barplots and count the frequency for each class. For a bivariate analysis it's been used a Chi-Square Test to evaluate the relationship between the target variable and each categorical feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71zfch9VfTYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "a62cd3c3-9dbf-4ab2-d696-63fee1294644"
      },
      "source": [
        "# let's have a look at how many labels for categorical features\n",
        "for col in X_full.columns:\n",
        "  if X_full[col].dtype ==\"object\":\n",
        "    print(col, ': ', len(X_full[col].unique()), ' labels')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique_ID :  4564  labels\n",
            "MORTGAGE_PURPOSE :  2  labels\n",
            "PAYMENT_FREQUENCY :  3  labels\n",
            "PROPERTY_TYPE :  8  labels\n",
            "FSA :  753  labels\n",
            "AGE_RANGE :  11  labels\n",
            "GENDER :  2  labels\n",
            "INCOME_TYPE :  16  labels\n",
            "NAICS CODE :  19  labels\n",
            "FSA_num :  10  labels\n",
            "FSA_let :  16  labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NREoC4kEBP17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
        "categorical_cols = [cname for cname in X_full.columns if\n",
        "                    X_full[cname].nunique() <= 15 and \n",
        "                    X_full[cname].dtype == \"object\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfZUySHMrLiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subset with categorical features\n",
        "cat = X_full[categorical_cols]\n",
        "cat.columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9notscfuyPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-sd8thFOoe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Univariate analysis with frequency and barplots\n",
        "sns.set( rc = {'figure.figsize': (5, 5)})\n",
        "fcat = ['MORTGAGE_PURPOSE','PAYMENT_FREQUENCY','PROPERTY_TYPE','AGE_RANGE','GENDER','FSA_num']\n",
        "\n",
        "for col in fcat:\n",
        "    count = pd.crosstab(index = cat[col], columns=\"count\")\n",
        "    percentage = pd.crosstab(index = cat[col], columns=\"frequency\")/pd.crosstab(index = cat[col], columns=\"frequency\").sum()\n",
        "    tab = pd.concat([count, percentage], axis=1)\n",
        "    plt.figure()\n",
        "    sns.countplot(x=cat[col], data=cat, palette=\"Set1\")\n",
        "    plt.xticks(rotation=45)\n",
        "    print(tab)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF4jl5aLOruj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bivariate analysis with barplots\n",
        "sns.set( rc = {'figure.figsize': (5, 5)})\n",
        "fcat = ['MORTGAGE_PURPOSE','PAYMENT_FREQUENCY','PROPERTY_TYPE','AGE_RANGE','GENDER','FSA_num']\n",
        "\n",
        "for col in fcat:\n",
        "    plt.figure()\n",
        "    sns.countplot(x=cat[col], hue=y, data=cat, palette=\"Set1\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t150D7qPOvpi",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3rr599qO05U",
        "colab_type": "text"
      },
      "source": [
        "The Chi-Square Test is used as feature selection testing the independence between target variable and categorical features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kUsmD_yO5_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Joined target variable with categorical features in a new data frame for a Chi-Square Test\n",
        "cat2 = pd.concat([y,cat], axis=1)\n",
        "testColumns = ['MORTGAGE_PURPOSE','PAYMENT_FREQUENCY','PROPERTY_TYPE','AGE_RANGE','GENDER','FSA_num']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oxN2AaLQcES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 0.05\n",
        "for var in testColumns:\n",
        "    X = cat2[var].astype(str)\n",
        "    Y = cat2['RESULT'].astype(str)\n",
        "    dfObserved = pd.crosstab(Y,X)\n",
        "    chi2, p, dof, expected = stats.chi2_contingency(dfObserved.values)\n",
        "    if p <= alpha:\n",
        "    \tprint('{0} Dependent (reject H0)'.format(var))\n",
        "    else:\n",
        "       print('{0} Independent (fail to reject H0)'.format(var))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cumBq9UUTh4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop features not helpful by Feature Selection\n",
        "cat = cat.drop(['GENDER','AGE_RANGE','FSA_num'], axis=1)\n",
        "cat.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHJVRnRmT7-j",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Engineering on categorical features: one-hot encoding into k-1 dummy variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSvpWEpNMQuJ",
        "colab_type": "text"
      },
      "source": [
        "Let's transform categorical features into numerical variables with one-hot encoding methodology to afford a better understanding of variables by machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7UsSPk3EdMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One-hot encode the data (to shorten the code, I use pandas)\n",
        "HOcat = pd.concat([pd.get_dummies(cat.MORTGAGE_PURPOSE, drop_first=True), pd.get_dummies(cat.PAYMENT_FREQUENCY, drop_first=True), pd.get_dummies(cat.PROPERTY_TYPE, drop_first=True)], axis=1)\n",
        "\n",
        "HOcat = HOcat.astype(int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsr5D7xTHNdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the new subset\n",
        "HOcat.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pUe0kjijI5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOcat.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cYuFQqAT5V0",
        "colab_type": "text"
      },
      "source": [
        "# Numerical Features Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab41k1QYT9O5",
        "colab_type": "text"
      },
      "source": [
        "#####- Analysis for numerical features (distribution, univariate analysis, bivariate analysis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtzxuguSMoTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select numerical columns\n",
        "numerical_cols = [cname for cname in X_full.columns if \n",
        "                X_full[cname].dtype in ['float64']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y565vj1nMobi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "31771f0b-f7cb-47b7-8d5f-ad33e1a1dbb4"
      },
      "source": [
        "# Subset with numerical features\n",
        "num = X_full[numerical_cols]\n",
        "num.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['PROPERTY_VALUE', 'MORTGAGE_PAYMENT', 'GDS', 'LTV', 'TDS',\n",
              "       'AMORTIZATION', 'MORTGAGE_AMOUNT', 'RATE', 'TERM', 'INCOME',\n",
              "       'CREDIT_SCORE'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKW0YwtqMtCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpbkECUAUKyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Univariate analysis with density plots and histograms\n",
        "sns.set( rc = {'figure.figsize': (5, 5)})\n",
        "fnum = ['PROPERTY_VALUE', 'MORTGAGE_PAYMENT','GDS', 'LTV', 'TDS', 'AMORTIZATION','MORTGAGE_AMOUNT', 'RATE', 'TERM', 'INCOME', 'CREDIT_SCORE']\n",
        "\n",
        "for col in fnum:\n",
        "    plt.figure()\n",
        "    x=num[col]\n",
        "    sns.distplot(x, bins=10, color='m')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZNHCWoFUN1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Univariate analysis with box-plots\n",
        "for col in fnum:\n",
        "    plt.figure()\n",
        "    x=num[col]\n",
        "    sns.boxplot(x,palette=\"Set1\",linewidth=1)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEDi7gGmUR3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Univariate analysis looking at Standard Deviation, Skewness and Kurtosis\n",
        "for col in fnum:\n",
        "  print(col,'\\nStandard Deviation :', stdev(num[col]), \n",
        "        '\\nSkewness :', skew(num[col]), \n",
        "        '\\nKurtosis :', kurtosis(num[col]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7-_kU2KUYGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bivariate analysis with box-plots\n",
        "for col in fnum:\n",
        "    plt.figure()\n",
        "    sns.violinplot(y=col, x=y, data=num, linewidth=2, palette=\"Set2\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9eyBWkrUiWP",
        "colab_type": "text"
      },
      "source": [
        "####- Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVsokoPvUnpv",
        "colab_type": "text"
      },
      "source": [
        "The Anova Test on the Logistic Regression is used as feature selection testing the importance of the predictor variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMR_adCKUm3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merging numerical covariates with dependent variable\n",
        "num2 = pd.concat([y,num], axis=1)\n",
        "num2['RESULT'] = np.where(num2['RESULT']=='FUNDED',1,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW9_ZTi4UvHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Anova Test on Logistic Regression\n",
        "formula = ('RESULT ~ PROPERTY_VALUE+MORTGAGE_PAYMENT+GDS+LTV+TDS+AMORTIZATION+MORTGAGE_AMOUNT+RATE+TERM+INCOME+CREDIT_SCORE')\n",
        "results = logit(formula=formula, data=num2).fit()\n",
        "results.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2GbcVMnKGaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop features not helpful by Feature Selection\n",
        "num = num.drop(['PROPERTY_VALUE','GDS','TDS','AMORTIZATION','MORTGAGE_AMOUNT','TERM','CREDIT_SCORE'], axis=1)\n",
        "num.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7x6UhSJvaT3",
        "colab_type": "text"
      },
      "source": [
        "####- Handling Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJvgq9fTveQx",
        "colab_type": "text"
      },
      "source": [
        "An outlier is an observation that is unlike the other observations. They are extreme values that fall far away of the other observations. There are several ways to handle outliers, the approach followed is to cap them within two boundaries based on 1.5 times the Inter Quartile Range (difference between 75th and 25th quartiles)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qy26s0oO8kJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def outliers_plot(dataframe, feature):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # histogram\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.distplot(dataframe[feature], bins=30, color='g')\n",
        "    plt.title('Histogram')\n",
        "    # Q-Q plot\n",
        "    plt.subplot(1, 3, 2)\n",
        "    stats.probplot(dataframe[feature], dist=\"norm\", plot=plt)\n",
        "    plt.ylabel('Variable quantiles')\n",
        "    # boxplot\n",
        "    plt.subplot(1, 3, 3)\n",
        "    x=dataframe[feature]\n",
        "    sns.boxplot(x,linewidth=1.5, color='g')\n",
        "    plt.title('Boxplot')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ubFeEu85YV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MORTGAGE_PAYMENT\n",
        "# before\n",
        "outliers_plot(num, 'MORTGAGE_PAYMENT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4nQjuTw5Ytp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# correction\n",
        "i = 'MORTGAGE_PAYMENT'\n",
        "q75, q25 = np.percentile(num[i].dropna(), [75 ,25])\n",
        "iqr = q75 - q25 \n",
        "min = q25 - (iqr*1.5)\n",
        "max = q75 + (iqr*1.5) \n",
        "num[i].loc[num[i] < min] = min\n",
        "num[i].loc[num[i] > max] = max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37XAcara5ZBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MORTGAGE_PAYMENT\n",
        "# after\n",
        "outliers_plot(num, 'MORTGAGE_PAYMENT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UzzThniPbQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LTV\n",
        "# before\n",
        "outliers_plot(num, 'LTV')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4XpszsDTp1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# correction\n",
        "i = 'LTV'\n",
        "q75, q25 = np.percentile(num[i].dropna(), [75 ,25])\n",
        "iqr = q75 - q25 \n",
        "min = q25 - (iqr*1.5)\n",
        "max = q75 + (iqr*1.5) \n",
        "num[i].loc[num[i] < min] = min\n",
        "num[i].loc[num[i] > max] = max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUyAaO4ATqlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# after\n",
        "outliers_plot(num, 'LTV')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXnR4_UpPxeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RATE\n",
        "# before\n",
        "outliers_plot(num, 'RATE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQAWcqW6UJEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# correction\n",
        "i = 'RATE'\n",
        "q75, q25 = np.percentile(num[i].dropna(), [75 ,25])\n",
        "iqr = q75 - q25 \n",
        "min = q25 - (iqr*1.5)\n",
        "max = q75 + (iqr*1.5) \n",
        "num[i].loc[num[i] < min] = min\n",
        "num[i].loc[num[i] > max] = max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvsPlNG3U77g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# after\n",
        "outliers_plot(num, 'RATE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWqne2L3Pxoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INCOME\n",
        "# before\n",
        "outliers_plot(num, 'INCOME')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H33yVE1uVG4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# correction\n",
        "i = 'INCOME'\n",
        "q75, q25 = np.percentile(num[i].dropna(), [75 ,25])\n",
        "iqr = q75 - q25 \n",
        "min = q25 - (iqr*1.5)\n",
        "max = q75 + (iqr*1.5) \n",
        "num[i].loc[num[i] < min] = min\n",
        "num[i].loc[num[i] > max] = max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqhm0G0zVHK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# after\n",
        "outliers_plot(num, 'INCOME')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd35nzi19GA3",
        "colab_type": "text"
      },
      "source": [
        "####- Feature engineering on numerical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY1f44WX9MO5",
        "colab_type": "text"
      },
      "source": [
        "Feature engineering is a fundamental step in the data science process, because with right features the job of modeling is much easier and predictive outcome will perform better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak88NtOZ9NUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a new feature \n",
        "num['X1'] = (num['RATE']/100)*(num['LTV']/100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VCNSIXf9iJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# before\n",
        "sns.distplot(num['INCOME'], bins=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a9AaWJA9rb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform a feature\n",
        "num['INCOME'] = np.log(num['INCOME'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfhtFsyf9riC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# after\n",
        "sns.distplot(num['INCOME'], bins=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdgA7nC29yVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# before\n",
        "sns.distplot(num['RATE'], bins=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMozkmHO9yYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discretization of \"RATE\" variable with K-means clustering\n",
        "disc = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cydzu50k9yar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "disc.fit(num[['RATE']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xToFqbvb9iRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_tf_k = disc.transform(num[['RATE']])\n",
        "num_tf_k = pd.DataFrame(num_tf_k, columns = ['RATE'])\n",
        "num_tf_k['RATE'] = num_tf_k['RATE'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO22-n5s9_eC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# after\n",
        "sns.countplot(num_tf_k['RATE'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5pyWJXg9_jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One-hot encode the 'RATE' feature (to shorten the code, I use pandas)\n",
        "HOnum = pd.concat([num[['MORTGAGE_PAYMENT','LTV','X1','INCOME']], \n",
        "                   pd.get_dummies(num_tf_k.RATE, prefix='RATE', drop_first=True)], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phfnirH1VAQI",
        "colab_type": "text"
      },
      "source": [
        "# Feature Selection on all data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xc8a1b-VEXW",
        "colab_type": "text"
      },
      "source": [
        "Another feature selection approach is to observe correlation between variables, let's apply it on all data set. There are some models such as linear regression where related features can deteriorate the performance (multicollinearity). Though some ensemble models are not sensitive at this topic, “Ensembles of tree-based models”, the approach followed is to remove them anyway because we don't know which model to use in advance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3u8LSBPMvij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grasp all\n",
        "X_all = pd.concat([HOcat, HOnum], axis=1, join='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKW8nxlCVHh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Correlation heatmap\n",
        "corr_matrix = X_all.corr()\n",
        "sns.set( rc = {'figure.figsize': (35, 35)})\n",
        "plt.figure()\n",
        "sns.heatmap(corr_matrix, square = True, annot=True, fmt='.2f')\n",
        "plt.title('Correlation Heatmap on data set',size=30)\n",
        "plt.yticks(fontsize=\"15\")\n",
        "plt.xticks(fontsize=\"15\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQIBYkTQVNHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select correlated features and removed it\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "# Find index of feature columns with correlation greater than 0.75\n",
        "to_drop = [column for column in upper.columns if any(upper[column].abs() > 0.75)]\n",
        "to_drop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7luqCCOVQqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop features \n",
        "X_all = X_all.drop(X_all[to_drop], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j724JmrLVl8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the new data set\n",
        "X_all.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTSkljECVSo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq_hhDqrr_-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look for constant variables and drop them\n",
        "for col in X_all.columns:\n",
        "  print(col,'\\nVariance :', np.var(X_all[col]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3g7VOS0w2lD",
        "colab_type": "text"
      },
      "source": [
        "# Split data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_Hw64zwxu_6",
        "colab_type": "text"
      },
      "source": [
        "To analyze the performance of a model is a good manner to split the data set into the training set and the test set. It's been decided to split it into three parts: training set, validation set and test set for a better understanding of models. The training set is a sample of data used to fit the model, meanwhile the validation set is a sample of data used to provide an unbiased evaluation of the model that fit on the training set and to tune the model hyperparameters (not in this explorative phase). The test set is a sample of data used to provide an unbiased evaluation of the model applied on data never seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csVOGV7Qw5VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Break off validation and test set from training data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y, train_size=0.8, test_size=0.2,\n",
        "                                                                random_state=0)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8, test_size=0.2,\n",
        "                                                                random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfvk77arw9dH",
        "colab_type": "text"
      },
      "source": [
        "# Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geBXV2Ah45YO",
        "colab_type": "text"
      },
      "source": [
        "Since values of the features are not uniform and may be neagatively impact the skill of some models, the same models are evaluated with a standardized copy of the data set. It means, data are transformed such that each feature has a mean value of 0 and a standard deviation of 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLARBi9ow-OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardization of data\n",
        "sc = StandardScaler()\n",
        "X_train_sc = sc.fit_transform(X_train)\n",
        "X_valid_sc = sc.fit_transform(X_valid)\n",
        "X_test_sc = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe16GLXqopQl",
        "colab_type": "text"
      },
      "source": [
        "# Modeling Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5VfzCYGcWgq",
        "colab_type": "text"
      },
      "source": [
        "The traditional data exploration is extended looking at the behaviour of several baseline models and which features can be relevant for the prediction. This exploration is splitted in two parts: without handling the imbalanced target variable (scaled baseline models) and handling it (scaled baseline models).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S3Jt8sxLF2b",
        "colab_type": "text"
      },
      "source": [
        "- Evaluation Metric and Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zebre5UP0u7",
        "colab_type": "text"
      },
      "source": [
        "The confusion matrix is a summary table representation of prediction results for a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. Good predictions coming from the higher diagonal values of the confusion matrix. For this imbalanced classification task is not used Accuracy metric but more appropriately the F1 score metric that combines both precision and recall, it's an harmonic mean between them, it's indicates how precise is the classifier (precision) and how robust it is (recall). F1 score equal to 0.00 indicates a poor model, instead F1 score equal 1.00 indicates a perfect model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQWGVT8QwwI8",
        "colab_type": "text"
      },
      "source": [
        "#  Modeling Part I: without handling imbalanced data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fHQdwCHxrIL",
        "colab_type": "text"
      },
      "source": [
        "The analysis is based on six baseline models: Logistic Regression as the easiest model and as well as benchmark, then other five models: Bagging, Random Forest, AdaBoost, Gradient Boosting Machine and Neural Networks (MLP)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwCD7aCLxEc6",
        "colab_type": "text"
      },
      "source": [
        "#####- Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d8vIEE4xFMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spot Check Algorithms\n",
        "models = []\n",
        "models.append(('LogisticRegression', LogisticRegression(random_state=0)))\n",
        "models.append(('Bagging', BaggingClassifier(random_state=0)))\n",
        "models.append(('RandomForest', RandomForestClassifier(random_state=0)))\n",
        "models.append(('AdaBoost', AdaBoostClassifier(random_state=0)))\n",
        "models.append(('GBM', GradientBoostingClassifier(random_state=0)))\n",
        "models.append(('NN', MLPClassifier(random_state=0)))\n",
        "results_tr = []\n",
        "results_v = []\n",
        "results_t = []\n",
        "names = []\n",
        "score = []\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "for (name, model) in models:\n",
        "    param_grid = {}\n",
        "    my_model = GridSearchCV(model,param_grid,cv=skf)\n",
        "    my_model.fit(X_train_sc, y_train)\n",
        "    predictions_tr = my_model.predict(X_train_sc) \n",
        "    predictions_v = my_model.predict(X_valid_sc)\n",
        "    predictions_t = my_model.predict(X_test_sc)\n",
        "    f1_train = f1_score(y_train, predictions_tr, average='macro') \n",
        "    f1_valid = f1_score(y_valid, predictions_v,average='macro') \n",
        "    f1_test = f1_score(y_test, predictions_t,average='macro') \n",
        "    results_tr.append(f1_train)\n",
        "    results_v.append(f1_valid)\n",
        "    results_t.append(f1_test) \n",
        "    names.append(name)\n",
        "    f_dict = {\n",
        "        'model': name,\n",
        "        'f1_train': f1_train,\n",
        "        'f1_valid': f1_valid,\n",
        "        'f1_test': f1_test\n",
        "    }\n",
        "    score.append(f_dict)\n",
        "    # Computing Confusion matrix for the above algorithms\n",
        "    sns.set( rc = {'figure.figsize': (5, 5)})\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix(my_model,X_test_sc, y_test,values_format= '.2f', cmap='Blues')\n",
        "    plt.title(name)\n",
        "    plt.show()   \n",
        "score = pd.DataFrame(score, columns = ['model','f1_train', 'f1_valid', 'f1_test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyNrSed6x_68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the F1 score for each model and for each data set\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBOAoq8kyDJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot results for a graphical comparison\n",
        "print(\"Spot Check Algorithms\")\n",
        "sns.set( rc = {'figure.figsize': (15, 5)})\n",
        "plt.figure()\n",
        "plt.subplot(1,3,1)  \n",
        "sns.stripplot(x=\"model\", y=\"f1_train\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Train results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.subplot(1,3,2)\n",
        "sns.stripplot(x=\"model\", y=\"f1_valid\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Validation results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.subplot(1,3,3)\n",
        "sns.stripplot(x=\"model\", y=\"f1_test\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Test results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKidas0TU-A6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Precision - Recall curves for a graphical comparison\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "# train models\n",
        "model_LR = LogisticRegression(random_state=0).fit(X_train_sc, y_train)\n",
        "model_BAG = BaggingClassifier(DecisionTreeClassifier(random_state=0)).fit(X_train_sc, y_train)\n",
        "model_RF = RandomForestClassifier(random_state=0).fit(X_train_sc, y_train)\n",
        "model_AB = AdaBoostClassifier(DecisionTreeClassifier(random_state=0)).fit(X_train_sc, y_train)\n",
        "model_GBM = GradientBoostingClassifier(random_state=0).fit(X_train_sc, y_train)\n",
        "model_NN = MLPClassifier(random_state=0).fit(X_train_sc, y_train)\n",
        "# precision-recall on train set\n",
        "axes = plt.gca()\n",
        "plot_precision_recall_curve(model_LR, X_train_sc, y_train, ax=axes)\n",
        "plot_precision_recall_curve(model_BAG, X_train_sc, y_train, ax=axes)\n",
        "plot_precision_recall_curve(model_RF, X_train_sc, y_train, ax=axes)\n",
        "plot_precision_recall_curve(model_AB, X_train_sc, y_train, ax=axes)\n",
        "plot_precision_recall_curve(model_GBM, X_train_sc, y_train, ax=axes)\n",
        "plot_precision_recall_curve(model_NN, X_train_sc, y_train, ax=axes)\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGMafnqCpLn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# precision-recall on validation set\n",
        "axes = plt.gca()\n",
        "plot_precision_recall_curve(model_LR, X_valid_sc, y_valid, ax=axes)\n",
        "plot_precision_recall_curve(model_BAG, X_valid_sc, y_valid, ax=axes)\n",
        "plot_precision_recall_curve(model_RF, X_valid_sc, y_valid, ax=axes)\n",
        "plot_precision_recall_curve(model_AB, X_valid_sc, y_valid, ax=axes)\n",
        "plot_precision_recall_curve(model_GBM, X_valid_sc, y_valid, ax=axes)\n",
        "plot_precision_recall_curve(model_NN, X_valid_sc, y_valid, ax=axes)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbq7a45ZpngU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# precision-recall on test set\n",
        "axes = plt.gca()\n",
        "plot_precision_recall_curve(model_LR, X_test_sc, y_test, ax=axes)\n",
        "plot_precision_recall_curve(model_BAG, X_test_sc, y_test, ax=axes)\n",
        "plot_precision_recall_curve(model_RF, X_test_sc, y_test, ax=axes)\n",
        "plot_precision_recall_curve(model_AB, X_test_sc, y_test, ax=axes)\n",
        "plot_precision_recall_curve(model_GBM, X_test_sc, y_test, ax=axes)\n",
        "plot_precision_recall_curve(model_NN, X_test_sc, y_test, ax=axes)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXxHROLstPqO",
        "colab_type": "text"
      },
      "source": [
        "# Oversampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxy6eUXcIMUt",
        "colab_type": "text"
      },
      "source": [
        "To overcome the imbalanced classification the simplest strategy is to apply a random resampling strategy. It's been used a random oversampling that consists on create a new transformed data set with randomly duplicate examples in the minority class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1Ls_-XYtMr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e79f374d-23bd-463e-93b2-63fa9938445e"
      },
      "source": [
        "# Oversampling\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
        "print(sorted(Counter(y_resampled).items()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('FUNDED', 2268), ('NOT FUNDED', 2268)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpbaValRtWJr",
        "colab_type": "text"
      },
      "source": [
        "# Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WdG0xBltq4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardization of data\n",
        "sc = StandardScaler()\n",
        "X_train_sc_re = sc.fit_transform(X_resampled)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bys_XRutMF5",
        "colab_type": "text"
      },
      "source": [
        "#  Modeling Part II: handling imbalanced data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSiQ0WyItWqy",
        "colab_type": "text"
      },
      "source": [
        "#####- Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCnacnuitrzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spot Check Algorithms\n",
        "models = []\n",
        "models.append(('LogisticRegression', LogisticRegression(random_state=0)))\n",
        "models.append(('Bagging', BaggingClassifier(random_state=0)))\n",
        "models.append(('RandomForest', RandomForestClassifier(random_state=0)))\n",
        "models.append(('AdaBoost', AdaBoostClassifier(random_state=0)))\n",
        "models.append(('GBM', GradientBoostingClassifier(random_state=0)))\n",
        "models.append(('NN', MLPClassifier(random_state=0)))\n",
        "results_tr = []\n",
        "results_v = []\n",
        "results_t = []\n",
        "names = []\n",
        "score = []\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "for (name, model) in models:\n",
        "    param_grid = {}\n",
        "    my_model = GridSearchCV(model,param_grid,cv=skf)\n",
        "    my_model.fit(X_train_sc_re, y_resampled)\n",
        "    predictions_tr = my_model.predict(X_train_sc_re) \n",
        "    predictions_v = my_model.predict(X_valid_sc)\n",
        "    predictions_t = my_model.predict(X_test_sc)\n",
        "    f1_train = f1_score(y_resampled, predictions_tr, average='macro') \n",
        "    f1_valid = f1_score(y_valid, predictions_v,average='macro') \n",
        "    f1_test = f1_score(y_test, predictions_t,average='macro') \n",
        "    results_tr.append(f1_train)\n",
        "    results_v.append(f1_valid)\n",
        "    results_t.append(f1_test)\n",
        "    \n",
        "    names.append(name)\n",
        "    f_dict = {\n",
        "        'model': name,\n",
        "        'f1_train': f1_train,\n",
        "        'f1_valid': f1_valid,\n",
        "        'f1_test': f1_test\n",
        "    }\n",
        "    score.append(f_dict)\n",
        "    # Computing Confusion matrix for the above algorithms\n",
        "    sns.set( rc = {'figure.figsize': (5, 5)})\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix(my_model,X_test_sc, y_test,values_format= '.2f', cmap='Greys')\n",
        "    plt.title(name)\n",
        "    plt.show()   \n",
        "score = pd.DataFrame(score, columns = ['model','f1_train', 'f1_valid', 'f1_test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESSL0TJ4txdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the F1 score for each model and for each data set\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOis5gkFt0pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot results for a graphical comparison\n",
        "print(\"Spot Check Algorithms\")\n",
        "sns.set( rc = {'figure.figsize': (15, 5)})\n",
        "plt.figure()\n",
        "plt.subplot(1,3,1)  \n",
        "sns.stripplot(x=\"model\", y=\"f1_train\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Train results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.subplot(1,3,2)\n",
        "sns.stripplot(x=\"model\", y=\"f1_valid\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Validation results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.subplot(1,3,3)\n",
        "sns.stripplot(x=\"model\", y=\"f1_test\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Test results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag7-IGs8IbeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zrVIl_o7Ib8s",
        "colab": {}
      },
      "source": [
        "# Plot Precision - Recall curves for a graphical comparison\n",
        "# train models\n",
        "model_LR_re = LogisticRegression(random_state=0).fit(X_train_sc_re, y_resampled)\n",
        "model_BAG_re = BaggingClassifier(DecisionTreeClassifier(random_state=0)).fit(X_train_sc_re, y_resampled)\n",
        "model_RF_re = RandomForestClassifier(random_state=0).fit(X_train_sc_re, y_resampled)\n",
        "model_AB_re = AdaBoostClassifier(DecisionTreeClassifier(random_state=0)).fit(X_train_sc_re, y_resampled)\n",
        "model_GBM_re = GradientBoostingClassifier(random_state=0).fit(X_train_sc_re, y_resampled)\n",
        "model_NN_re = MLPClassifier(random_state=0).fit(X_train_sc_re, y_resampled)\n",
        "# precision-recall on train set\n",
        "axes = plt.gca()\n",
        "plot_precision_recall_curve(model_LR_re, X_train_sc_re, y_resampled, ax=axes)\n",
        "plot_precision_recall_curve(model_BAG_re, X_train_sc_re, y_resampled, ax=axes)\n",
        "plot_precision_recall_curve(model_RF_re, X_train_sc_re, y_resampled, ax=axes)\n",
        "plot_precision_recall_curve(model_AB_re, X_train_sc_re, y_resampled, ax=axes)\n",
        "plot_precision_recall_curve(model_GBM_re, X_train_sc_re, y_resampled, ax=axes)\n",
        "plot_precision_recall_curve(model_NN_re, X_train_sc_re, y_resampled, ax=axes)\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IExQfsIoJYQd",
        "colab": {}
      },
      "source": [
        "# precision-recall on validation set\n",
        "axes = plt.gca()\n",
        "plot_precision_recall_curve(model_LR_re, X_valid_sc, y_valid, ax=axes)\n",
        "plot_precision_recall_curve(model_BAG_re, X_valid_sc, y_valid, ax=axes)\n",
        "plot_precision_recall_curve(model_RF_re, X_valid_sc, y_valid, ax=axes)\n",
        "plot_precision_recall_curve(model_AB_re, X_valid_sc, y_valid, ax=axes)\n",
        "plot_precision_recall_curve(model_GBM_re, X_valid_sc, y_valid, ax=axes)\n",
        "plot_precision_recall_curve(model_NN_re, X_valid_sc, y_valid, ax=axes)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l6Sf0w4wJdYs",
        "colab": {}
      },
      "source": [
        "# precision-recall on test set\n",
        "axes = plt.gca()\n",
        "plot_precision_recall_curve(model_LR_re, X_test_sc, y_test, ax=axes)\n",
        "plot_precision_recall_curve(model_BAG_re, X_test_sc, y_test, ax=axes)\n",
        "plot_precision_recall_curve(model_RF_re, X_test_sc, y_test, ax=axes)\n",
        "plot_precision_recall_curve(model_AB_re, X_test_sc, y_test, ax=axes)\n",
        "plot_precision_recall_curve(model_GBM_re, X_test_sc, y_test, ax=axes)\n",
        "plot_precision_recall_curve(model_NN_re, X_test_sc, y_test, ax=axes)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pD7DcP6tcoT",
        "colab_type": "text"
      },
      "source": [
        "# Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP6oSaletg9l",
        "colab_type": "text"
      },
      "source": [
        "Feature importance answers to this question \"What features have the biggest impact on predictions?\" Are showed feature importance from models, by permutation or by SHAP values. Permutation is based on understand the magnitude of each feature shuffling it per time and looking how the prediction change. SHAP values exploits the game thoery to measure how much each feature in the model contributes, either positively or negatively, to a predicted score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wORztoYwtk3U",
        "colab_type": "text"
      },
      "source": [
        "### ***Logistic Regression***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEqwdNHsc6P2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Top relevant features Logistic Regression\n",
        "#model_LR = LogisticRegression(random_state=0)\n",
        "#model_LR.fit(X_train_sc_re, y_resampled)\n",
        "feature_importances = model_LR_re.coef_[0]\n",
        "\n",
        "(pd.Series(feature_importances, index=X_train.columns).nlargest(10).plot(kind='bar')).set_title(\"Logistic Regression\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ebJnqZt5ku",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Permutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTlv-1dit8NZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Permutation importance Logistic Regression\n",
        "print('Logistic Regression Features Permutation Importance')\n",
        "perm = PermutationImportance(model_LR_re, random_state=0).fit(X_train_sc_re, y_resampled)\n",
        "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWzOVzrBt-SO",
        "colab_type": "text"
      },
      "source": [
        "### ***Bagging***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nfWy25KuDl3",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NEV7mdwuMd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Top relevant features Bagging\n",
        "#model_BAG = BaggingClassifier(DecisionTreeClassifier(random_state=0))\n",
        "#model_BAG.fit(X_train_sc_re, y_resampled)\n",
        "feature_importances = np.mean([\n",
        "    tree.feature_importances_ for tree in model_BAG_re.estimators_\n",
        "], axis=0)\n",
        "\n",
        "(pd.Series(feature_importances, index=X_train.columns).nlargest(10).plot(kind='bar')).set_title(\"Bagging\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zGbCDSJuGzi",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Permutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6nA9sUJuNuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Permutation importance Bagging\n",
        "print('Bagging Features Permutation Importance')\n",
        "perm = PermutationImportance(model_BAG_re, random_state=0).fit(X_train_sc_re, y_resampled)\n",
        "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTXwGnlguJNt",
        "colab_type": "text"
      },
      "source": [
        "### ***Random Forest***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4YgGiwWuUdO",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_DIoe5FuXce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Top relevant features RF\n",
        "#model_RF = RandomForestClassifier()\n",
        "#model_RF.fit(X_train_sc_re, y_resampled)\n",
        "(pd.Series(model_RF_re.feature_importances_, index=X_train.columns).nlargest(10).plot(kind='bar')).set_title(\"RF Features Importance\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRquIOnjuad3",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Permutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Amb8D4mudNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Permutation importance RF\n",
        "print('RF Features Permutation Importance')\n",
        "perm = PermutationImportance(model_RF_re, random_state=0).fit(X_train_sc_re, y_resampled)\n",
        "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQT_KTxDufyu",
        "colab_type": "text"
      },
      "source": [
        "### ***AdaBoost***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9U50UBpuiRF",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBmrWS8GulWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Top relevant features AdaBoost\n",
        "#model_AB = AdaBoostClassifier(DecisionTreeClassifier(random_state=0))\n",
        "#model_AB.fit(X_train_sc_re, y_resampled)\n",
        "feature_importances = np.mean([\n",
        "    tree.feature_importances_ for tree in model_AB_re.estimators_\n",
        "], axis=0)\n",
        "\n",
        "(pd.Series(feature_importances, index=X_train.columns).nlargest(10).plot(kind='bar')).set_title(\"AdaBoost\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWPKLjg4uoPZ",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Permutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUz6bvIMuv9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Permutation importance AdaBoost\n",
        "print('AdaBoost Features Permutation Importance')\n",
        "perm = PermutationImportance(model_AB_re, random_state=0).fit(X_train_sc_re, y_resampled)\n",
        "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkfgOypVus8d",
        "colab_type": "text"
      },
      "source": [
        "### ***GBM***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTyODrAVuz_D",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22MXIpervY29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Top relevant features GBM\n",
        "#model_GBM = GradientBoostingClassifier()\n",
        "#model_GBM.fit(X_train_sc_re, y_resampled)\n",
        "(pd.Series(model_GBM_re.feature_importances_, index=X_train.columns).nlargest(10).plot(kind='bar')).set_title(\"GBM Features Importance\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geNVXNASu0KK",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Permutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyBMDPFpvcG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Permutation importance GBM\n",
        "print('GBM Features Permutation Importance')\n",
        "perm = PermutationImportance(model_GBM_re, random_state=0).fit(X_train_sc_re, y_resampled)\n",
        "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYAKtAiZu0UL",
        "colab_type": "text"
      },
      "source": [
        "#####- SHAP Values Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-IGAupLvee1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SHAP Values Interpretability GBM\n",
        "print(\"GBM SHAP Values\")\n",
        "shap_values = shap.TreeExplainer(model_GBM_re).shap_values(X_train_sc_re)\n",
        "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQd4obe5viGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shap.summary_plot(shap_values, features= X_train_sc_re, feature_names=X_train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jR-y_dyvip_",
        "colab_type": "text"
      },
      "source": [
        "### ***NN***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGgTRr8ZvlKY",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Permutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01MsUBVfvnoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Permutation importance NN\n",
        "print('NN Features Permutation Importance')\n",
        "#model_NN = MLPClassifier(random_state=0)\n",
        "#model_NN.fit(X_train_sc_re, y_resampled)\n",
        "perm = PermutationImportance(model_NN_re, random_state=0).fit(X_train_sc_re, y_resampled)\n",
        "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}