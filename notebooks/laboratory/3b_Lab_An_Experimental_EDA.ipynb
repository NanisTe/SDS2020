{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3b_Lab_An_Experimental_EDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudio1975/SDS2020/blob/master/notebooks/laboratory/3b_Lab_An_Experimental_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlcrvwrX70Uv",
        "colab_type": "text"
      },
      "source": [
        "# **An Experimental Exploratory Data Analysis for a Classification Task step 3**\n",
        "\n",
        "### ***From Visualization to Statistical Analysis***\n",
        "\n",
        "### ***From Feature Engineering to Feature Selection***\n",
        "\n",
        "### ***From the Best Model Selection to Interpretability***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vngHrhnzA7qr",
        "colab_type": "text"
      },
      "source": [
        "To start the exploration set up the environment with libraries, upload the data set (it's stored in a github repository) and split it into target variable and features variables. No more set up is required using Google Colab. Look at the guidelines: https://colab.research.google.com/notebooks/welcome.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2o9mujoz5by",
        "colab_type": "text"
      },
      "source": [
        "#### **Contents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRwDUZ019MxF",
        "colab_type": "text"
      },
      "source": [
        "The goal of this challenge, launched by CrowdAnalytix, is to develop a model to predict whether a mortgage will be funded or not based on certain factors in a customer’s application data. \n",
        "The evaluation metric used is the F1 score.\n",
        "The data set is made up by 45.642 observations with predictor variables (21 features) and the target variable. It's a classification task with the goal to predict the 'Result' target variable for every row (Funded, Not Funded). Look at the competition: https://www.crowdanalytix.com/contests/propensity-to-fund-mortgages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9DkyQZ2Ls-4",
        "colab_type": "text"
      },
      "source": [
        "### **Exploratory Data Analysis (EDA) Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVQahUb-8995",
        "colab_type": "text"
      },
      "source": [
        "![](http://www.theleader.info/wp-content/uploads/2017/07/Mortgage-rates.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs9lEhRM0Mne",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJlI9RQG-A8w",
        "colab_type": "text"
      },
      "source": [
        "#####- Upload libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQysnLU9RUKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload libraries\n",
        "\n",
        "# to handle data set\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# to plot\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "# statistics\n",
        "import statistics\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from scipy.stats import chi2_contingency\n",
        "from scipy.stats import kurtosis \n",
        "from scipy.stats import skew\n",
        "from statistics import stdev \n",
        "\n",
        "# to split data set \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# to build models\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# to evaluate models\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# to handle imbalanced data set\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "\n",
        "# feature engineering\n",
        "!pip install feature-engine\n",
        "import feature_engine\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# feature importance\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "!pip install eli5 \n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "!pip install shap\n",
        "import shap\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLFLLtY2-I06",
        "colab_type": "text"
      },
      "source": [
        "#####- Upload data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgQMY8HqDrGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload dataset\n",
        "url = 'https://raw.githubusercontent.com/claudio1975/SDS2020/master/data/CAX_train_small.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2_fumrJ-NCl",
        "colab_type": "text"
      },
      "source": [
        "#####- Split data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwPBNz5pDvDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data set between target and features\n",
        "X_full = df\n",
        "y = X_full.RESULT\n",
        "X_full = X_full.drop(['RESULT'], axis=1)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIEjdLM1BGti",
        "colab_type": "text"
      },
      "source": [
        "# Summarize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvXmGWII7I_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at dimension of data set and types of each attribute\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDB8l79ZLldm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summarize attribute distributions of the data frame\n",
        "df.describe(include='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl_waIVssaPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take a peek at the first rows of the data\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6N4eR9vEXQi",
        "colab_type": "text"
      },
      "source": [
        "Explanatory variables are grouped into categorical variables and numerical variables and for each one let's do a graphical and non-graphical analysis, but before this split let's run  some data preparation activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqhoHvdpPKse",
        "colab_type": "text"
      },
      "source": [
        "# Formatting Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbHzfBaj2J5t",
        "colab_type": "text"
      },
      "source": [
        "If necessary, it's a good practice to format data, after have taken a peek of it. Missing values on numeric features are marked by \"-1\", meanwhile for categorical features they are marked with \"Unknown\"; let's imput these values with \"NA\".  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoC1p8FWLFbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replaced both '-1' and 'Unknown' values with NA's\n",
        "X_full[X_full== -1] = np.nan\n",
        "X_full[X_full==\"Unknown\"] = np.nan"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfr3gw7vQeWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Format data into float and object types and split mixed variables\n",
        "X_full['PROPERTY VALUE'] = X_full['PROPERTY VALUE'].astype(float)\n",
        "X_full['MORTGAGE PAYMENT'] = X_full['MORTGAGE PAYMENT'].astype(float)\n",
        "X_full['AMORTIZATION'] = X_full['AMORTIZATION'].astype(float)\n",
        "X_full['TERM'] = X_full['TERM'].astype(float)\n",
        "X_full['INCOME'] = X_full['INCOME'].astype(float)\n",
        "X_full['INCOME TYPE'] = X_full['INCOME TYPE'].astype(object)\n",
        "X_full['CREDIT SCORE'] = X_full['CREDIT SCORE'].astype(float)\n",
        "X_full['FSA_num'] = X_full['FSA'].str.extract('(\\d+)') # extract numerical part\n",
        "X_full['FSA_let'] = X_full['FSA'].str[0] # extract the first letter"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa67IEEHGdPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rename some features for a practical use\n",
        "X_full = X_full.rename(columns={\"MORTGAGE PURPOSE\":\"MORTGAGE_PURPOSE\",\"PAYMENT FREQUENCY\":\"PAYMENT_FREQUENCY\",\"PROPERTY TYPE\":\"PROPERTY_TYPE\",\"AGE RANGE\":\"AGE_RANGE\",\"PROPERTY VALUE\": \"PROPERTY_VALUE\",\n",
        "                                \"MORTGAGE PAYMENT\": \"MORTGAGE_PAYMENT\", \"MORTGAGE AMOUNT\":\"MORTGAGE_AMOUNT\",\"INCOME TYPE\":\"INCOME_TYPE\",\"CREDIT SCORE\":\"CREDIT_SCORE\"})"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJDCLUOUl-dv",
        "colab_type": "text"
      },
      "source": [
        "# Handling missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6F1o-jDnAp_",
        "colab_type": "text"
      },
      "source": [
        "There are two categorical features with missing values lower than 40%. The approach followed: fill up missing values with the last observation carried forward (LOCF) and for both of them let's creat a boolean feature with 1 (true-missing value) or 0 (false-actual value). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPRuA3QAmGr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check missing values both to numeric features and categorical features\n",
        "X_full.isnull().sum()/X_full.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "espKgZhvnJiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input missing values with the last observation carried forward (LOCF)\n",
        "X_full['GENDER'].fillna(method='ffill', inplace=True)\n",
        "X_full['GENDER'] = X_full['GENDER'].astype(object)\n",
        "X_full['INCOME_TYPE'].fillna(method='ffill', inplace=True)\n",
        "X_full['INCOME_TYPE'] = X_full['INCOME_TYPE'].astype(object)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEut5Q20nL6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# final check\n",
        "X_full.isnull().sum()/X_full.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQcW8rYbOFt6",
        "colab_type": "text"
      },
      "source": [
        "# Target Variable Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6lQf-z8OIIl",
        "colab_type": "text"
      },
      "source": [
        "The target variable is grouped into two classes: \"Funded\" and \"Not Funded\". Looking at the barplot, it's quite imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfJ91jxjOLJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summarize the class distribution \n",
        "count = pd.crosstab(index = y, columns=\"count\")\n",
        "percentage = pd.crosstab(index = y, columns=\"frequency\")/pd.crosstab(index = y, columns=\"frequency\").sum()\n",
        "pd.concat([count, percentage], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EahRBbD7QTOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Plot the target variable\n",
        "ax = sns.countplot(x=y, data=X_full, palette='rocket', order=[\"FUNDED\", \"NOT FUNDED\"]).set_title(\"Target Variable Distribution\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqcTsDB4OkMK",
        "colab_type": "text"
      },
      "source": [
        "# Categorical Features Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mod4f0ZSOhnk",
        "colab_type": "text"
      },
      "source": [
        "#####- Analysis for categorical features (barplot, univariate analysis, bivariate analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZQKBWaMOaLB",
        "colab_type": "text"
      },
      "source": [
        "Let's group all categorical features into a new subset: let's run a graphical analysis using barplots and count the frequency for each class. For a bivariate analysis it's been used a Chi-Square Test to evaluate the relationship between the target variable and each categorical feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ikq5RxVHueV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's have a look at how many labels for categorical features\n",
        "for col in X_full.columns:\n",
        "  if X_full[col].dtype ==\"object\":\n",
        "    print(col, ': ', len(X_full[col].unique()), ' labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NREoC4kEBP17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
        "categorical_cols = [cname for cname in X_full.columns if\n",
        "                    X_full[cname].nunique() <= 15 and \n",
        "                    X_full[cname].dtype == \"object\"]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfZUySHMrLiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subset with categorical features\n",
        "cat = X_full[categorical_cols]\n",
        "cat.columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9notscfuyPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-sd8thFOoe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Univariate analysis with frequency and barplots\n",
        "sns.set( rc = {'figure.figsize': (5, 5)})\n",
        "fcat = ['MORTGAGE_PURPOSE','PAYMENT_FREQUENCY','PROPERTY_TYPE','AGE_RANGE','GENDER','FSA_num']\n",
        "\n",
        "for col in fcat:\n",
        "    count = pd.crosstab(index = cat[col], columns=\"count\")\n",
        "    percentage = pd.crosstab(index = cat[col], columns=\"frequency\")/pd.crosstab(index = cat[col], columns=\"frequency\").sum()\n",
        "    tab = pd.concat([count, percentage], axis=1)\n",
        "    plt.figure()\n",
        "    sns.countplot(x=cat[col], data=cat, palette=\"deep\")\n",
        "    plt.xticks(rotation=45)\n",
        "    print(tab)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF4jl5aLOruj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bivariate analysis with barplots\n",
        "sns.set( rc = {'figure.figsize': (5, 5)})\n",
        "fcat = ['MORTGAGE_PURPOSE','PAYMENT_FREQUENCY','PROPERTY_TYPE','AGE_RANGE','GENDER','FSA_num']\n",
        "\n",
        "for col in fcat:\n",
        "    plt.figure()\n",
        "    sns.countplot(x=cat[col], hue=y, data=cat, palette=\"Set2\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ai-DOi7REik",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQikgmPZRA8z",
        "colab_type": "text"
      },
      "source": [
        "The Chi-Square Test is used as feature selection testing the null hypothesis of independence between target variable and categorical features. The goal is to test that two classifications are independent or not. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vLpFFHKRAEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Joined target variable with categorical features in a new data frame for a Chi-Square Test\n",
        "cat2 = pd.concat([y,cat], axis=1)\n",
        "testColumns = ['MORTGAGE_PURPOSE','PAYMENT_FREQUENCY','PROPERTY_TYPE','AGE_RANGE','GENDER','FSA_num']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6t3fjiPQ9tH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 0.05\n",
        "for var in testColumns:\n",
        "    X = cat2[var].astype(str)\n",
        "    Y = cat2['RESULT'].astype(str)\n",
        "    dfObserved = pd.crosstab(Y,X)\n",
        "    chi2, p, dof, expected = stats.chi2_contingency(dfObserved.values)\n",
        "    if p <= alpha:\n",
        "    \tprint('{0} Dependent (reject H0)'.format(var))\n",
        "    else:\n",
        "       print('{0} Independent (fail to reject H0)'.format(var))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF9QuOF9ROmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop features not helpful by Feature Selection\n",
        "cat = cat.drop(['GENDER', 'AGE_RANGE','FSA_num'], axis=1)\n",
        "cat.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHJVRnRmT7-j",
        "colab_type": "text"
      },
      "source": [
        "#####- Feature Engineering on categorical features: label encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSvpWEpNMQuJ",
        "colab_type": "text"
      },
      "source": [
        "Let's transform categorical features into numerical variables with label encoding methodology to afford a better understanding of variables by machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7UsSPk3EdMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "for col in cat.columns:\n",
        "  cat[col] = cat[col].astype(str)\n",
        "# Make copy to avoid changing original data \n",
        "label_cat = cat.copy()\n",
        "# Apply label encoder to each column with categorical data\n",
        "label_encoder = LabelEncoder()\n",
        "for col in label_cat.columns:\n",
        "    label_cat[col] = label_encoder.fit_transform(label_cat[col])\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsr5D7xTHNdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the new subset\n",
        "label_cat.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pUe0kjijI5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_cat.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cYuFQqAT5V0",
        "colab_type": "text"
      },
      "source": [
        "# Numerical Features Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab41k1QYT9O5",
        "colab_type": "text"
      },
      "source": [
        "#####- Analysis for numerical features (distribution, univariate analysis, bivariate analysis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtzxuguSMoTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select numerical columns\n",
        "numerical_cols = [cname for cname in X_full.columns if \n",
        "                X_full[cname].dtype in ['float64']]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y565vj1nMobi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subset with numerical features\n",
        "num = X_full[numerical_cols]\n",
        "num.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKW0YwtqMtCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpbkECUAUKyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Univariate analysis with density plots and histograms\n",
        "sns.set( rc = {'figure.figsize': (5, 5)})\n",
        "fnum = ['PROPERTY_VALUE', 'MORTGAGE_PAYMENT','GDS', 'LTV', 'TDS', 'AMORTIZATION','MORTGAGE_AMOUNT', 'RATE', 'TERM', 'INCOME', 'CREDIT_SCORE']\n",
        "\n",
        "for col in fnum:\n",
        "    plt.figure()\n",
        "    x=num[col]\n",
        "    sns.distplot(x, hist=False, rug=False, color='b')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZNHCWoFUN1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Univariate analysis with box-plots\n",
        "for col in fnum:\n",
        "    plt.figure()\n",
        "    x=num[col]\n",
        "    sns.stripplot(x,palette=\"Set2\",jitter=True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEDi7gGmUR3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Univariate analysis looking at Standard Deviation, Skewness and Kurtosis\n",
        "for col in fnum:\n",
        "  print(col,'\\nStandard Deviation :', stdev(num[col]), \n",
        "        '\\nSkewness :', skew(num[col]), \n",
        "        '\\nKurtosis :', kurtosis(num[col]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7-_kU2KUYGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bivariate analysis with box-plots\n",
        "for col in fnum:\n",
        "    plt.figure()\n",
        "    sns.stripplot(y=col, x=y, data=num, jitter=True, palette=\"Blues\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9eyBWkrUiWP",
        "colab_type": "text"
      },
      "source": [
        "####- Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVsokoPvUnpv",
        "colab_type": "text"
      },
      "source": [
        "Bivariate analysis with Kendall's Test can be used as feature selection testing the null hypothesis of independence between target variable and numerical features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMR_adCKUm3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Kendall's Test \n",
        "num2 = pd.concat([y,num], axis=1)\n",
        "num2['RESULT'] = np.where(num2['RESULT']=='FUNDED',1,0)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdNQFj_VCQ8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 0.05\n",
        "for var in fnum:\n",
        "  p = stats.kendalltau(num2['RESULT'],num2[var])[1]\n",
        "  if p <= alpha:\n",
        "    \tprint('{0} Dependent (reject H0)'.format(var))\n",
        "  else:\n",
        "       print('{0} Independent (fail to reject H0)'.format(var))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhDVmiNgH-jA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop features not helpful by Feature Selection\n",
        "num = num.drop(['TERM','CREDIT_SCORE'], axis=1)\n",
        "num.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phfnirH1VAQI",
        "colab_type": "text"
      },
      "source": [
        "# Feature Selection on all data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xc8a1b-VEXW",
        "colab_type": "text"
      },
      "source": [
        "Another feature selection approach is to observe correlation between variables, let's apply it on all data set. There are some models such as linear regression where related features can deteriorate the performance (multicollinearity). Though some ensemble models are not sensitive at this topic, “Ensembles of tree-based models”, the approach followed is to remove them anyway because we don't know which model to use in advance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3u8LSBPMvij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grasp all\n",
        "X_all = pd.concat([label_cat, num], axis=1, join='inner')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKW8nxlCVHh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Correlation heatmap\n",
        "corr_matrix = X_all.corr()\n",
        "sns.set( rc = {'figure.figsize': (35, 35)})\n",
        "plt.figure()\n",
        "sns.heatmap(corr_matrix, square = True, annot=True, fmt='.2f')\n",
        "plt.title('Correlation Heatmap on data set',size=30)\n",
        "plt.yticks(fontsize=\"15\")\n",
        "plt.xticks(fontsize=\"15\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQIBYkTQVNHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select correlated features and removed it\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "# Find index of feature columns with correlation greater than 0.75\n",
        "to_drop = [column for column in upper.columns if any(upper[column].abs() > 0.75)]\n",
        "to_drop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7luqCCOVQqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop features \n",
        "X_all = X_all.drop(X_all[to_drop], axis=1)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j724JmrLVl8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the new data set\n",
        "X_all.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTSkljECVSo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt-W9nCDf2K_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look for constant variables and drop them\n",
        "for col in X_all.columns:\n",
        "  print(col,'\\nVariance :', np.var(X_all[col]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3g7VOS0w2lD",
        "colab_type": "text"
      },
      "source": [
        "# Split data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_Hw64zwxu_6",
        "colab_type": "text"
      },
      "source": [
        "To analyze the performance of a model is a good manner to split the data set into the training set and the test set. It's been decided to split it into three parts: training set, validation set and test set for a better understanding of models. The training set is a sample of data used to fit the model, meanwhile the validation set is a sample of data used to provide an unbiased evaluation of the model that fit on the training set and to tune the model hyperparameters (not in this explorative phase). The test set is a sample of data used to provide an unbiased evaluation of the model applied on data never seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csVOGV7Qw5VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Break off validation and test set from training data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y, train_size=0.8, test_size=0.2,\n",
        "                                                                random_state=0)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8, test_size=0.2,\n",
        "                                                                random_state=0)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe16GLXqopQl",
        "colab_type": "text"
      },
      "source": [
        "# Modeling Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5VfzCYGcWgq",
        "colab_type": "text"
      },
      "source": [
        "The traditional data exploration is extended looking at the behaviour of several baseline models and which features can be relevant for the prediction. This exploration is splitted in two parts: without handling the imbalanced target variable (scaled baseline models) and handling it (scaled baseline models).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S3Jt8sxLF2b",
        "colab_type": "text"
      },
      "source": [
        "- Evaluation Metric and Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zebre5UP0u7",
        "colab_type": "text"
      },
      "source": [
        "The confusion matrix is a summary table representation of prediction results for a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. Good predictions coming from the higher diagonal values of the confusion matrix. For this imbalanced classification task is not used Accuracy metric but more appropriately the F1 score metric that combines both precision and recall, it's an harmonic mean between them, it's indicates how precise is the classifier (precision) and how robust it is (recall). F1 score equal to 0.00 indicates a poor model, instead F1 score equal 1.00 indicates a perfect model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfvk77arw9dH",
        "colab_type": "text"
      },
      "source": [
        "# Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geBXV2Ah45YO",
        "colab_type": "text"
      },
      "source": [
        "Since values of the features are not uniform and may be neagatively impact the skill of some models, the same models are evaluated with a standardized copy of the data set. It means, data are transformed such that each feature has a mean value of 0 and a standard deviation of 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLARBi9ow-OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardization of data\n",
        "sc = StandardScaler()\n",
        "X_train_sc = sc.fit_transform(X_train)\n",
        "X_valid_sc = sc.fit_transform(X_valid)\n",
        "X_test_sc = sc.transform(X_test)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQWGVT8QwwI8",
        "colab_type": "text"
      },
      "source": [
        "#  Modeling Part I: without handling imbalanced data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fHQdwCHxrIL",
        "colab_type": "text"
      },
      "source": [
        "The analysis is based on six baseline models: Logistic Regression as the easiest model and as well as benchmark, then other five models: Bagging, Random Forest, AdaBoost, Gradient Boosting Machine and Neural Networks (MLP)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwCD7aCLxEc6",
        "colab_type": "text"
      },
      "source": [
        "#####- Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d8vIEE4xFMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spot Check Algorithms\n",
        "models = []\n",
        "models.append(('LogisticRegression', LogisticRegression(random_state=0)))\n",
        "models.append(('Bagging', BaggingClassifier(random_state=0)))\n",
        "models.append(('RandomForest', RandomForestClassifier(random_state=0)))\n",
        "models.append(('AdaBoost', AdaBoostClassifier(random_state=0)))\n",
        "models.append(('GBM', GradientBoostingClassifier(random_state=0)))\n",
        "models.append(('NN', MLPClassifier(random_state=0)))\n",
        "results_tr = []\n",
        "results_v = []\n",
        "results_t = []\n",
        "names = []\n",
        "score = []\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "for (name, model) in models:\n",
        "    param_grid = {}\n",
        "    my_model = GridSearchCV(model,param_grid,cv=skf)\n",
        "    my_model.fit(X_train_sc, y_train)\n",
        "    predictions_tr = my_model.predict(X_train_sc) \n",
        "    predictions_v = my_model.predict(X_valid_sc)\n",
        "    predictions_t = my_model.predict(X_test_sc)\n",
        "    f1_train = f1_score(y_train, predictions_tr, average='macro') \n",
        "    f1_valid = f1_score(y_valid, predictions_v,average='macro') \n",
        "    f1_test = f1_score(y_test, predictions_t,average='macro') \n",
        "    results_tr.append(f1_train)\n",
        "    results_v.append(f1_valid)\n",
        "    results_t.append(f1_test)\n",
        "    \n",
        "    names.append(name)\n",
        "    f_dict = {\n",
        "        'model': name,\n",
        "        'f1_train': f1_train,\n",
        "        'f1_valid': f1_valid,\n",
        "        'f1_test': f1_test\n",
        "    }\n",
        "    score.append(f_dict)\n",
        "    # Computing Confusion matrix for the above algorithms\n",
        "    sns.set( rc = {'figure.figsize': (5, 5)})\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix(my_model,X_test_sc, y_test,values_format= '.2f', cmap='Blues')\n",
        "    plt.title(name)\n",
        "    plt.show()   \n",
        "score = pd.DataFrame(score, columns = ['model','f1_train', 'f1_valid', 'f1_test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyNrSed6x_68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the F1 score for each model and for each data set\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBOAoq8kyDJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot results for a graphical comparison\n",
        "print(\"Spot Check Algorithms\")\n",
        "sns.set( rc = {'figure.figsize': (15, 5)})\n",
        "plt.figure()\n",
        "plt.subplot(1,3,1)  \n",
        "sns.stripplot(x=\"model\", y=\"f1_train\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Train results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.subplot(1,3,2)\n",
        "sns.stripplot(x=\"model\", y=\"f1_valid\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Validation results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.subplot(1,3,3)\n",
        "sns.stripplot(x=\"model\", y=\"f1_test\",data=score,size=15)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Test results')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1.1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}